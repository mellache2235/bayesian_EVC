# Running Model Comparisons: Original vs. Arithmetic Data

## Quick Answer

**YES**, the same comparison script works for both datasets!

**But** you need to specify which dataset to use.

---

## Two Datasets Available

### **Dataset 1: Original Behavioral Data**
```
File: data/behavioral_data.csv
Generated by: step1_generate_data.py
Description: Generic behavioral task (reward-based decision-making)
N: 30 subjects × 200 trials = 6000 trials
```

### **Dataset 2: Arithmetic Task Data**
```
File: data/arithmetic/arithmetic_task_data.csv
Generated by: generate_arithmetic_data.py
Description: Children solving math problems
N: 30 children × 100 trials = 3000 trials
```

---

## Running Comparisons on Original Data

### **Full Pipeline:**

```bash
# Step 1: Generate original data
python3 step1_generate_data.py

# Step 2: Estimate uncertainty
python3 step2_estimate_uncertainty.py

# Step 3: Fit Traditional EVC
python3 step3_fit_traditional_evc.py

# Step 4: Fit Bayesian EVC (no temporal)
python3 step4_fit_bayesian_evc.py

# Step 4b: Fit Temporal Bayesian EVC (with history)
python3 step4b_fit_temporal_bayesian_evc.py

# Step 5: Compare all three models
python3 step5_compare_all_models.py
```

**This uses:** `data/behavioral_data.csv`

---

## Running Comparisons on Arithmetic Data

### **Option A: Use Original Comparison Script (Requires Modification)**

The `step5_compare_all_models.py` is currently hardcoded to use `data/behavioral_data.csv`.

To use arithmetic data, you need to modify line 36:

```python
# Current (line 36):
data = pd.read_csv('data/behavioral_data.csv')

# Change to:
data = pd.read_csv('data/arithmetic/arithmetic_task_data.csv')
```

**Then run:**
```bash
python3 step5_compare_all_models.py
```

---

### **Option B: Use Dedicated Arithmetic Script (Recommended)**

The `fit_bayesian_evc_arithmetic.py` already compares non-temporal vs. temporal for arithmetic data.

```bash
# Generate arithmetic data
python3 generate_arithmetic_data.py

# Fit and compare models
python3 fit_bayesian_evc_arithmetic.py
```

**But** this doesn't include Traditional EVC comparison.

---

## Solution: Universal Comparison Script

Let me create a version that works for BOTH datasets:

### **New Script: `compare_models_universal.py`**

```bash
# For original data:
python3 compare_models_universal.py --data behavioral

# For arithmetic data:
python3 compare_models_universal.py --data arithmetic
```

This is more flexible and cleaner!

---

## Column Name Mapping

**Important:** The two datasets have different column names!

### **Original Data Columns:**
```python
'subject_id'           # Subject identifier
'trial'                # Trial number
'control_signal'       # Observed control (what we predict)
'reward_magnitude'     # Reward for trial
'evidence_clarity'     # Accuracy/quality
'total_uncertainty'    # Combined uncertainty
'accuracy'             # Trial outcome (0/1)
```

### **Arithmetic Data Columns:**
```python
'child_id'             # Child identifier → maps to 'subject_id'
'trial'                # Trial number (same)
'control_signal'       # Observed control (same)
'reward'               # Reward for problem → maps to 'reward_magnitude'
'expected_accuracy'    # Expected success rate → maps to 'evidence_clarity'
'total_uncertainty'    # Combined uncertainty (same)
'correct'              # Trial outcome → maps to 'accuracy'
```

**The comparison scripts need to handle this mapping!**

---

## Current Status of Scripts

### **✅ Works for Original Data:**
- `step1_generate_data.py` → Generates data
- `step2_estimate_uncertainty.py` → Adds uncertainty
- `step3_fit_traditional_evc.py` → Fits Traditional EVC
- `step4_fit_bayesian_evc.py` → Fits Bayesian EVC
- `step4b_fit_temporal_bayesian_evc.py` → Fits Temporal EVC
- `step5_compare_all_models.py` → **Compares all 3 models ✓**

### **✅ Works for Arithmetic Data:**
- `generate_arithmetic_data.py` → Generates data
- `fit_bayesian_evc_arithmetic.py` → Fits Bayesian + Temporal (only 2 models)

### **❌ Missing:**
- Universal script that works for both
- Traditional EVC comparison for arithmetic data

---

## Recommendation

I'll create two things:

### **1. Universal Comparison Script**
Works for both datasets with command-line argument

### **2. Column Mapper**
Automatically handles different column names

Want me to create these now?

---

## Quick Workaround (If You Want to Run Now)

### **For Original Data:**
```bash
# Just run step5
python3 step5_compare_all_models.py
```

### **For Arithmetic Data:**

**Manual approach:**

1. **Rename columns in arithmetic data:**
```python
import pandas as pd

data = pd.read_csv('data/arithmetic/arithmetic_task_data.csv')

# Rename to match original data format
data = data.rename(columns={
    'child_id': 'subject_id',
    'reward': 'reward_magnitude',
    'expected_accuracy': 'evidence_clarity',
    'correct': 'accuracy'
})

# Save as compatible format
data.to_csv('data/arithmetic/arithmetic_data_compatible.csv', index=False)
```

2. **Modify `step5_compare_all_models.py` line 36:**
```python
data = pd.read_csv('data/arithmetic/arithmetic_data_compatible.csv')
```

3. **Run:**
```bash
python3 step5_compare_all_models.py
```

---

## What I'll Create for You

### **New File: `compare_models_universal.py`**

```python
"""
Universal model comparison script
Works for both behavioral and arithmetic datasets
"""

import argparse
import pandas as pd

def load_data(dataset_type):
    """Load and normalize column names"""
    
    if dataset_type == 'behavioral':
        data = pd.read_csv('data/behavioral_data.csv')
        # Column names already correct
        
    elif dataset_type == 'arithmetic':
        data = pd.read_csv('data/arithmetic/arithmetic_task_data.csv')
        # Rename columns to match
        data = data.rename(columns={
            'child_id': 'subject_id',
            'reward': 'reward_magnitude',
            'expected_accuracy': 'evidence_clarity',
            'correct': 'accuracy'
        })
    
    else:
        raise ValueError(f"Unknown dataset type: {dataset_type}")
    
    return data

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('--data', type=str, default='behavioral',
                       choices=['behavioral', 'arithmetic'],
                       help='Which dataset to use')
    args = parser.parse_args()
    
    # Load data
    data = load_data(args.data)
    
    # Run comparison (same code for both!)
    # ... rest of comparison code ...
```

**Usage:**
```bash
python3 compare_models_universal.py --data behavioral
python3 compare_models_universal.py --data arithmetic
```

---

## Summary

### **Current Situation:**

**Original Data:**
- ✅ Full pipeline works (steps 1-5)
- ✅ Compares all 3 models
- ✅ Run: `python3 step5_compare_all_models.py`

**Arithmetic Data:**
- ✅ Data generation works
- ✅ Compares 2 models (Bayesian + Temporal)
- ❌ Doesn't compare Traditional EVC
- ❌ Need to modify scripts or use workaround

---

### **What You Should Do:**

**Option 1: Quick (Use Existing Scripts)**
```bash
# For original data - works perfectly
python3 step5_compare_all_models.py
```

**Option 2: I Create Universal Script (Better)**
- Works for both datasets
- Clean command-line interface
- No manual modifications needed

**Which do you prefer?**

Want me to create the universal comparison script now?


