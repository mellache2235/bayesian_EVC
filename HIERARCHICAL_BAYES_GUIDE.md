# Hierarchical Bayesian Modeling for Small Sample Sizes

## The Small Sample Problem

### **Typical In-House Study:**
- **N = 15-30 subjects** (realistic for fMRI/EEG)
- **200-400 trials per subject**
- **Total: 3,000-12,000 trials**

### **The Challenge:**

With small N, you face a dilemma:

#### **Option 1: Pool All Data (Current Approach)**
```python
# Fit one set of parameters across all subjects
model.fit(all_data)
# Result: Œ≤_r = 0.93, Œ≤_e = 1.95, Œª = 0.41
```

**Problems:**
- ‚ùå Ignores individual differences
- ‚ùå Parameters are "average" but don't fit anyone well
- ‚ùå Poor generalization to new subjects
- ‚ùå Can't study individual variability

**Example:**
```
Subject 1: High reward sensitivity (Œ≤_r = 1.5)
Subject 2: Low reward sensitivity (Œ≤_r = 0.3)
Pooled model: Œ≤_r = 0.9 (fits neither well!)
```

---

#### **Option 2: Fit Each Subject Separately**
```python
# Fit separate parameters for each subject
for subject in subjects:
    subject_params = model.fit(subject_data)
```

**Problems:**
- ‚ùå Only 200 trials per subject ‚Üí unstable estimates
- ‚ùå Some subjects have extreme/unrealistic parameters
- ‚ùå Can't borrow strength across subjects
- ‚ùå Can't make population-level inferences

**Example:**
```
Subject 1: 200 trials ‚Üí Œ≤_r = 2.3 (unstable)
Subject 2: 200 trials ‚Üí Œ≤_r = -0.5 (impossible!)
Subject 3: 200 trials ‚Üí Œ≤_r = 0.8 (reasonable)
```

---

## üéØ Solution: Hierarchical Bayesian Modeling

### **Key Idea: Partial Pooling**

**"Borrow strength across subjects while respecting individual differences"**

Instead of:
- Complete pooling (everyone identical)
- No pooling (everyone independent)

Do:
- **Partial pooling** (everyone similar but unique)

---

## How Hierarchical Bayes Works

### **The Model Structure:**

```
POPULATION LEVEL (Group)
    ‚Üì
    Hyperparameters describe the distribution
    Œº_Œ≤r = 1.0  (mean reward sensitivity)
    œÉ_Œ≤r = 0.3  (variability across people)
    
INDIVIDUAL LEVEL (Subject)
    ‚Üì
    Each subject's parameters drawn from population
    Subject 1: Œ≤_r ~ Normal(Œº_Œ≤r, œÉ_Œ≤r) ‚Üí Œ≤_r = 1.2
    Subject 2: Œ≤_r ~ Normal(Œº_Œ≤r, œÉ_Œ≤r) ‚Üí Œ≤_r = 0.8
    Subject 3: Œ≤_r ~ Normal(Œº_Œ≤r, œÉ_Œ≤r) ‚Üí Œ≤_r = 1.1
    
TRIAL LEVEL (Observation)
    ‚Üì
    Behavior generated from individual parameters
    Trial 1: control ~ f(Œ≤_r, reward, uncertainty, ...)
```

---

### **Mathematical Formulation:**

#### **Level 1: Population (Hyperpriors)**

```
Œº_Œ≤r ~ Normal(1.0, 0.5)      # Mean reward sensitivity
œÉ_Œ≤r ~ HalfNormal(0.3)       # Between-subject variability

Œº_Œ≤e ~ Normal(1.0, 0.5)      # Mean effort cost
œÉ_Œ≤e ~ HalfNormal(0.3)

Œº_Œª ~ Normal(0.5, 0.3)       # Mean uncertainty weight
œÉ_Œª ~ HalfNormal(0.2)

Œº_baseline ~ Normal(0.5, 0.2)
œÉ_baseline ~ HalfNormal(0.1)
```

**Interpretation:**
- Œº parameters: "What's typical for humans?"
- œÉ parameters: "How much do people differ?"

---

#### **Level 2: Individual (Subject-specific parameters)**

```
For each subject i:

Œ≤_r[i] ~ Normal(Œº_Œ≤r, œÉ_Œ≤r)
Œ≤_e[i] ~ Normal(Œº_Œ≤e, œÉ_Œ≤e)
Œª[i] ~ Normal(Œº_Œª, œÉ_Œª)
baseline[i] ~ Normal(Œº_baseline, œÉ_baseline)
```

**Interpretation:**
- Each subject gets their own parameters
- But they're "pulled" toward the group mean
- Amount of pulling depends on data quality

---

#### **Level 3: Trial (Observations)**

```
For each trial t of subject i:

control[i,t] ~ Normal(predicted_control[i,t], œÉ_obs)

where:
predicted_control[i,t] = baseline[i] + 
    (Œ≤_r[i] √ó reward[t] √ó accuracy[t] + Œª[i] √ó uncertainty[t]) / 
    (2 √ó Œ≤_e[i])
```

**Interpretation:**
- Observed control comes from model prediction + noise
- Each subject uses their own parameters

---

## üîë The Magic: Shrinkage

### **How Partial Pooling Works:**

When you have **limited data** for a subject, their estimate is **pulled toward the group mean**.

When you have **lots of data** for a subject, their estimate **stays close to their individual fit**.

---

### **Visual Example:**

```
Subject with 50 trials (little data):
‚îú‚îÄ Individual-only estimate: Œ≤_r = 2.5 (unstable, extreme)
‚îú‚îÄ Group mean: Œ≤_r = 1.0
‚îî‚îÄ Hierarchical estimate: Œ≤_r = 1.3 (shrunk toward mean)
    ‚îî‚îÄ "We're not sure about this subject, so trust the group"

Subject with 500 trials (lots of data):
‚îú‚îÄ Individual-only estimate: Œ≤_r = 1.8 (stable)
‚îú‚îÄ Group mean: Œ≤_r = 1.0
‚îî‚îÄ Hierarchical estimate: Œ≤_r = 1.7 (close to individual)
    ‚îî‚îÄ "We have good data, trust the individual estimate"
```

---

### **Mathematical Shrinkage:**

The hierarchical estimate is a weighted average:

```
Œ≤_r[hierarchical] = w √ó Œ≤_r[individual] + (1-w) √ó Œº_Œ≤r[group]

where:
w = n_trials / (n_trials + k)
k = œÉ_Œ≤r¬≤ / œÉ_obs¬≤  (ratio of between-subject to within-subject variance)
```

**Interpretation:**
- More trials ‚Üí w closer to 1 ‚Üí trust individual
- Fewer trials ‚Üí w closer to 0 ‚Üí trust group
- Automatic and optimal!

---

## üìä Comparison: Standard vs. Hierarchical

### **Scenario: 20 subjects, 200 trials each**

| Approach | Parameter Estimates | Generalization | Individual Differences | Sample Size Needed |
|----------|-------------------|----------------|----------------------|-------------------|
| **Pooled** | 1 set for all | Poor | Ignored | Large (1000+ trials) |
| **Individual** | 20 sets (unstable) | Very poor | Captured but noisy | Very large (500+ trials/subject) |
| **Hierarchical** | 20 sets (stable) + group | **Good** | **Captured + stabilized** | **Small (50-200 trials/subject)** |

---

### **Example Results:**

#### **Pooled Model:**
```
Œ≤_r = 0.93 ¬± 0.05
Test R¬≤ = -0.03
```
- Single value doesn't fit anyone well
- Can't explain individual differences

---

#### **Individual Model:**
```
Subject 1: Œ≤_r = 2.31 ¬± 0.82  (unstable!)
Subject 2: Œ≤_r = -0.15 ¬± 0.91 (impossible!)
Subject 3: Œ≤_r = 0.78 ¬± 0.45
...
Subject 20: Œ≤_r = 1.52 ¬± 0.67

Mean: Œ≤_r = 1.03 ¬± 0.71
Test R¬≤ = -0.15  (overfitting!)
```
- Extreme estimates
- High uncertainty
- Poor generalization

---

#### **Hierarchical Model:**
```
GROUP LEVEL:
Œº_Œ≤r = 1.02 ¬± 0.08  (population mean)
œÉ_Œ≤r = 0.31 ¬± 0.06  (between-subject SD)

INDIVIDUAL LEVEL:
Subject 1: Œ≤_r = 1.35 ¬± 0.12  (stabilized!)
Subject 2: Œ≤_r = 0.72 ¬± 0.14  (reasonable!)
Subject 3: Œ≤_r = 0.89 ¬± 0.11
...
Subject 20: Œ≤_r = 1.18 ¬± 0.13

Test R¬≤ = 0.24  (much better!)
```
- Reasonable estimates
- Lower uncertainty
- Better generalization
- Can make population inferences!

---

## üí° Why Hierarchical Bayes is Perfect for Small N

### **1. Efficient Use of Data**

With 20 subjects √ó 200 trials:
- **Pooled:** Uses 4,000 trials to estimate 4 parameters ‚Üí wasteful
- **Individual:** Uses 200 trials to estimate 4 parameters √ó 20 ‚Üí unstable
- **Hierarchical:** Uses 4,000 trials to estimate 4 group + 80 individual parameters ‚Üí optimal!

---

### **2. Regularization**

Extreme estimates are automatically "shrunk" toward reasonable values:

```
Subject with noisy data:
‚îú‚îÄ Raw estimate: Œ≤_r = 3.2 (probably noise)
‚îî‚îÄ Hierarchical: Œ≤_r = 1.4 (regularized toward group mean)
```

This is **automatic** and **data-driven** (not arbitrary)!

---

### **3. Handles Missing Data**

If a subject has very few trials:
```python
Subject 1: 200 trials ‚Üí Œ≤_r = 1.2 ¬± 0.15
Subject 2: 50 trials  ‚Üí Œ≤_r = 1.0 ¬± 0.25  (closer to group mean)
Subject 3: 10 trials  ‚Üí Œ≤_r = 1.0 ¬± 0.35  (very close to group mean)
```

The model **automatically adjusts** confidence based on data quality!

---

### **4. Population Inference**

You can answer questions like:
- "What's the typical uncertainty weight in humans?" ‚Üí Œº_Œª = 0.42
- "How much do people vary?" ‚Üí œÉ_Œª = 0.18
- "Is this parameter reliably different from zero?" ‚Üí P(Œº_Œª > 0) = 0.998

**This is impossible with individual-only models!**

---

### **5. Prediction for New Subjects**

When a new subject arrives:
```python
# Use population distribution
new_subject_Œ≤r ~ Normal(Œº_Œ≤r, œÉ_Œ≤r)

# As they do trials, update their individual estimate
# Starts at group mean, moves toward individual value
```

This is **ideal for clinical applications**!

---

## üõ†Ô∏è Implementation in Python

### **Option 1: PyMC (Recommended)**

```python
import pymc as pm
import numpy as np

def fit_hierarchical_evc(data):
    """
    Fit hierarchical Bayesian EVC model
    
    Parameters:
    -----------
    data : DataFrame with columns
        - subject_id
        - control (observed)
        - reward
        - accuracy
        - uncertainty
    """
    
    # Prepare data
    n_subjects = data['subject_id'].nunique()
    subject_idx = data['subject_id'].astype('category').cat.codes.values
    
    control_obs = data['control'].values
    reward = data['reward'].values
    accuracy = data['accuracy'].values
    uncertainty = data['uncertainty'].values
    
    with pm.Model() as hierarchical_model:
        
        # ============================================
        # LEVEL 1: POPULATION HYPERPARAMETERS
        # ============================================
        
        # Mean parameters across population
        mu_baseline = pm.Normal('mu_baseline', mu=0.5, sigma=0.2)
        mu_beta_r = pm.Normal('mu_beta_r', mu=1.0, sigma=0.5)
        mu_beta_e = pm.Normal('mu_beta_e', mu=1.0, sigma=0.5)
        mu_lambda = pm.Normal('mu_lambda', mu=0.5, sigma=0.3)
        
        # Between-subject variability
        sigma_baseline = pm.HalfNormal('sigma_baseline', sigma=0.1)
        sigma_beta_r = pm.HalfNormal('sigma_beta_r', sigma=0.3)
        sigma_beta_e = pm.HalfNormal('sigma_beta_e', sigma=0.3)
        sigma_lambda = pm.HalfNormal('sigma_lambda', sigma=0.2)
        
        # ============================================
        # LEVEL 2: INDIVIDUAL SUBJECT PARAMETERS
        # ============================================
        
        # Each subject's parameters (non-centered parameterization)
        baseline_offset = pm.Normal('baseline_offset', mu=0, sigma=1, 
                                    shape=n_subjects)
        beta_r_offset = pm.Normal('beta_r_offset', mu=0, sigma=1, 
                                  shape=n_subjects)
        beta_e_offset = pm.Normal('beta_e_offset', mu=0, sigma=1, 
                                  shape=n_subjects)
        lambda_offset = pm.Normal('lambda_offset', mu=0, sigma=1, 
                                  shape=n_subjects)
        
        # Transform to actual parameters
        baseline = pm.Deterministic('baseline', 
            mu_baseline + baseline_offset * sigma_baseline)
        beta_r = pm.Deterministic('beta_r', 
            mu_beta_r + beta_r_offset * sigma_beta_r)
        beta_e = pm.Deterministic('beta_e', 
            mu_beta_e + beta_e_offset * sigma_beta_e)
        lambda_param = pm.Deterministic('lambda', 
            mu_lambda + lambda_offset * sigma_lambda)
        
        # ============================================
        # LEVEL 3: TRIAL-LEVEL PREDICTIONS
        # ============================================
        
        # Expected value (traditional EVC component)
        expected_value = reward * accuracy
        
        # Predicted control for each trial
        # Uses subject-specific parameters via subject_idx
        predicted_control = (
            baseline[subject_idx] + 
            (beta_r[subject_idx] * expected_value + 
             lambda_param[subject_idx] * uncertainty) / 
            (2 * beta_e[subject_idx])
        )
        
        # Observation noise
        sigma_obs = pm.HalfNormal('sigma_obs', sigma=0.2)
        
        # Likelihood
        control_likelihood = pm.Normal('control_obs', 
                                       mu=predicted_control,
                                       sigma=sigma_obs,
                                       observed=control_obs)
        
        # ============================================
        # INFERENCE
        # ============================================
        
        # Sample from posterior
        trace = pm.sample(
            2000,  # Number of samples
            tune=1000,  # Burn-in
            chains=4,  # Parallel chains
            target_accept=0.95,  # Higher for better sampling
            return_inferencedata=True
        )
    
    return trace, hierarchical_model


# ============================================
# USAGE
# ============================================

# Fit model
trace, model = fit_hierarchical_evc(behavioral_data)

# ============================================
# EXTRACT RESULTS
# ============================================

# Population-level parameters
print("POPULATION-LEVEL ESTIMATES:")
print(f"Mean uncertainty weight: {trace.posterior['mu_lambda'].mean():.3f}")
print(f"  95% CI: [{trace.posterior['mu_lambda'].quantile(0.025):.3f}, "
      f"{trace.posterior['mu_lambda'].quantile(0.975):.3f}]")
print(f"Between-subject SD: {trace.posterior['sigma_lambda'].mean():.3f}")

# Individual parameters
print("\nINDIVIDUAL ESTIMATES:")
for i in range(n_subjects):
    lambda_i = trace.posterior['lambda'][:, :, i].values.flatten()
    print(f"Subject {i+1}: Œª = {lambda_i.mean():.3f} ¬± {lambda_i.std():.3f}")

# Probability that uncertainty weight > 0
prob_positive = (trace.posterior['mu_lambda'] > 0).mean()
print(f"\nP(Œª > 0) = {prob_positive:.4f}")
```

---

### **Key Features of This Implementation:**

#### **1. Non-Centered Parameterization**

```python
# Instead of:
beta_r[i] ~ Normal(mu_beta_r, sigma_beta_r)  # Can be slow

# Use:
beta_r_offset[i] ~ Normal(0, 1)
beta_r[i] = mu_beta_r + beta_r_offset[i] * sigma_beta_r
```

**Why:** Much faster sampling, especially with small N!

---

#### **2. Automatic Shrinkage**

The model automatically determines how much to shrink based on:
- Number of trials per subject
- Quality of data
- Between-subject variability (œÉ parameters)

**No manual tuning needed!**

---

#### **3. Full Uncertainty Quantification**

You get:
- Point estimates (posterior means)
- Uncertainty (posterior SDs)
- Credible intervals (95% CI)
- Probability statements (P(Œª > 0))

---

#### **4. Diagnostics**

```python
import arviz as az

# Check convergence
az.plot_trace(trace, var_names=['mu_lambda', 'sigma_lambda'])

# Check R-hat (should be < 1.01)
az.summary(trace, var_names=['mu_lambda', 'mu_beta_r', 'mu_beta_e'])

# Posterior predictive checks
ppc = pm.sample_posterior_predictive(trace, model=model)
az.plot_ppc(ppc)
```

---

## üìà Expected Improvements with Hierarchical Bayes

### **With N=20 subjects, 200 trials each:**

| Metric | Pooled | Individual | Hierarchical |
|--------|--------|------------|--------------|
| **Test R¬≤** | -0.03 | -0.15 | **0.20-0.35** |
| **RMSE** | 0.19 | 0.23 | **0.14-0.16** |
| **Correlation** | 0.36 | 0.28 | **0.50-0.65** |
| **Parameter Stability** | N/A | Poor | **Good** |
| **Generalization** | Poor | Very poor | **Good** |

**Expected improvement: 5-10x better R¬≤ with small N!**

---

## üéØ When to Use Hierarchical Bayes

### **Use Hierarchical Bayes When:**

‚úÖ **Small sample size** (N < 50 subjects)
‚úÖ **Repeated measures** (multiple trials per subject)
‚úÖ **Individual differences matter** (clinical, personality)
‚úÖ **Want population inferences** ("typical human")
‚úÖ **Need stable estimates** (for prediction, clinical use)
‚úÖ **Missing data** (some subjects have fewer trials)

---

### **Use Pooled/Individual When:**

‚ùå **Very large N** (N > 500 subjects, single trial each)
‚ùå **No repeated measures** (one observation per subject)
‚ùå **Computational constraints** (need fast fitting)
‚ùå **Exploratory analysis** (quick prototyping)

**For EVC with fMRI/EEG: Hierarchical Bayes is almost always better!**

---

## üíª Practical Workflow

### **Step 1: Start with Pooled Model (Quick Check)**

```python
# Quick sanity check
from models.bayesian_evc import BayesianEVC

model = BayesianEVC()
results = model.fit(data)
print(f"Pooled Œª = {results['uncertainty_weight']:.3f}")
```

**Purpose:** Verify data quality, check if uncertainty matters at all

---

### **Step 2: Fit Hierarchical Model (Main Analysis)**

```python
# Full hierarchical model
trace, model = fit_hierarchical_evc(data)

# Population inference
print(f"Population Œª = {trace.posterior['mu_lambda'].mean():.3f}")
print(f"P(Œª > 0) = {(trace.posterior['mu_lambda'] > 0).mean():.4f}")
```

**Purpose:** Get stable estimates, population inference, individual parameters

---

### **Step 3: Validate with Cross-Validation**

```python
# Leave-one-subject-out cross-validation
from sklearn.model_selection import LeaveOneOut

loo = LeaveOneOut()
predictions = []

for train_subjects, test_subject in loo.split(subjects):
    # Fit on N-1 subjects
    trace_train = fit_hierarchical_evc(data[data['subject_id'].isin(train_subjects)])
    
    # Predict held-out subject using population distribution
    # (they start at group mean, then update with their data)
    pred = predict_new_subject(trace_train, data[data['subject_id'] == test_subject])
    predictions.append(pred)

# Evaluate
r2_loo = r2_score(true_values, predictions)
print(f"LOO R¬≤ = {r2_loo:.3f}")
```

**Purpose:** Test generalization to new subjects

---

### **Step 4: Visualize Individual Differences**

```python
import matplotlib.pyplot as plt
import seaborn as sns

# Extract individual uncertainty weights
lambda_individual = trace.posterior['lambda'].mean(dim=['chain', 'draw']).values

# Plot
plt.figure(figsize=(10, 6))
plt.hist(lambda_individual, bins=15, alpha=0.7, edgecolor='black')
plt.axvline(trace.posterior['mu_lambda'].mean(), 
            color='red', linestyle='--', linewidth=2,
            label=f"Population mean = {trace.posterior['mu_lambda'].mean():.3f}")
plt.xlabel('Uncertainty Weight (Œª)', fontsize=12)
plt.ylabel('Number of Subjects', fontsize=12)
plt.title('Individual Differences in Uncertainty Sensitivity', fontsize=14)
plt.legend()
plt.tight_layout()
plt.savefig('individual_differences.png', dpi=300)
```

---

## üî¨ Clinical Applications

### **Example: Depression Study**

```python
# Compare healthy controls vs. depressed patients

# Add group indicator
data['group'] = data['subject_id'].map(subject_to_group)  # 0=control, 1=depressed

with pm.Model() as clinical_model:
    
    # Separate population means for each group
    mu_lambda_control = pm.Normal('mu_lambda_control', mu=0.5, sigma=0.3)
    mu_lambda_depressed = pm.Normal('mu_lambda_depressed', mu=0.5, sigma=0.3)
    
    # Shared between-subject variability
    sigma_lambda = pm.HalfNormal('sigma_lambda', sigma=0.2)
    
    # Individual parameters
    lambda_offset = pm.Normal('lambda_offset', mu=0, sigma=1, shape=n_subjects)
    
    # Group-specific means
    mu_lambda = pm.math.switch(group[subject_idx], 
                                mu_lambda_depressed, 
                                mu_lambda_control)
    
    lambda_param = mu_lambda + lambda_offset * sigma_lambda
    
    # ... rest of model ...
    
    trace = pm.sample(2000, tune=1000, chains=4)

# Test group difference
lambda_diff = trace.posterior['mu_lambda_depressed'] - trace.posterior['mu_lambda_control']
print(f"Difference: {lambda_diff.mean():.3f}")
print(f"P(depressed > control) = {(lambda_diff > 0).mean():.4f}")
```

**Interpretation:**
- If P(depressed > control) > 0.95 ‚Üí depressed patients are more sensitive to uncertainty
- Can relate to clinical symptoms, treatment response, etc.

---

## üìö Summary

### **Why Hierarchical Bayes for Small N:**

1. **Efficient:** Uses all data optimally
2. **Stable:** Regularizes extreme estimates
3. **Flexible:** Handles missing data, unbalanced designs
4. **Informative:** Population + individual inferences
5. **Predictive:** Generalizes to new subjects
6. **Realistic:** Matches actual cognitive neuroscience sample sizes

### **Bottom Line:**

**For N=15-30 subjects with 200 trials each:**

- ‚ùå Pooled model: Ignores individual differences, R¬≤ ‚âà 0
- ‚ùå Individual model: Unstable, overfits, R¬≤ < 0
- ‚úÖ **Hierarchical model: Stable, generalizes, R¬≤ = 0.2-0.4**

**Hierarchical Bayes is the gold standard for small-N cognitive neuroscience!**

---

## üöÄ Next Steps

1. **Implement hierarchical model** (use code above)
2. **Compare to pooled model** (should see 5-10x improvement)
3. **Validate with LOO cross-validation**
4. **Visualize individual differences**
5. **Test on real data** (OpenNeuro datasets)

**This will make your paper much stronger!** üìÑ‚ú®


